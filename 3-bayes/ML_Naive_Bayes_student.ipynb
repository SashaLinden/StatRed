{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF-SR: LabExercise Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sr] data spamham.csv already downloaded\n",
      "[sr] data biometrie2014.csv already downloaded\n",
      "Using Python version 3.8.10 (default, Nov 22 2023, 10:22:35) \n",
      "[GCC 9.4.0]\n",
      "Using nltk version 3.4.5\n",
      "Please install nltk version 3.8.1 in case you encounter any problems in grading exercises using this package\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sasha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/sasha/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sasha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import nltk\n",
    "import os\n",
    "import wget\n",
    "import sys\n",
    "\n",
    "def sr_download(file):\n",
    "    if not os.path.isfile(file):\n",
    "        url = \"https://staff.fnwi.uva.nl/r.vandenboomgaard/downloads/\" + file\n",
    "        print('[sr]',f'Downloading file {file} for statistical reasoning course')\n",
    "        wget.download(url)\n",
    "    else:\n",
    "        print('[sr]',f'data {file} already downloaded')\n",
    "        \n",
    "sr_download('spamham.csv')\n",
    "sr_download('biometrie2014.csv')\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "print(f\"Using Python version {sys.version}\")\n",
    "print(f\"Using nltk version {nltk.__version__}\")\n",
    "if nltk.__version__ < \"3.5\": \n",
    "    print(\"Please install nltk version 3.8.1 in case you encounter any problems in grading exercises using this package\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection we will look at a naive Bayes classifier for text messages. First a text message is reduced to a list of words (out of a fixed length dictionary of $n$ words). The occurrence of a word (with index $j$ in the dictionary) in a text message is indicated with the boolean random variable $W_i$ for $i=1,\\ldots,n$. A text message is classified either as 'spam' or 'ham' (non spam). The class is indicated with the random varianble $Y$ that is also a boolean random variable ($Y=1$ for 'spam' and $Y=0$ for 'ham')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bayes classifier then is:\n",
    "$$\\text{classify}(w_1, \\ldots, w_n) = \\arg\\max_{Y=y} P(Y=y \\bigm| W_1=w_1, \\ldots, W_n=w_n) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite the classifier using the naive Bayes assumption and express the classifier in terms of the class conditional probabilities $P(W_i=w_i\\bigm|Y=y)$ and a priori probabilities $P(Y=y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-bb65a39104c63a54",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "$$\\text{classify}(w_1,\\ldots,w_n) = \\arg\\max_y P(Y=y) \\prod_{i=1}^{n} P(W_i=w_i\\bigm| Y=y)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will analyze a data set with short texts labeled as either spam or ham (not spam). The goal is to develop a classifier that is able to do the classification given only the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reading and displaying the data we will use the ``csv`` Python module (note that Numpy can read csv files but it is hard to deal with variable length strings in Numpy). For processing the text messages we will use the ``nltk`` module. Some methods from natural language processing are needed to deal with the text messages. These methods allow us to represent the text messages as numerical information to be used in numerical machine learning techniques (the naive Bayesian classifier in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('spamham.csv')\n",
    "reader = csv.reader(file, delimiter='\\t')\n",
    "data = [row for row in reader]\n",
    "s = f'| |**class**| **text**|\\n| -- | :--- | :--- |\\n'\n",
    "for i in range(10):\n",
    "    s += f'| {i} |{data[i][0]}|{data[i][1]}|\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ``data`` is not a numpy array but a list of lists. Instead of using ``data[k,0]`` to get at the label in the k-th row we have to write ``data[k][0]``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the dataset is done by:\n",
    "- making all characters lowercase, \n",
    "- splitting the text into words and omitting punctuation,\n",
    "- removing stopwords (the commonly used words like 'the' because they are not characteristic for either spam or ham messages), and\n",
    "- stemming (i.e. replacing a word by its 'stem', i.e. the wordt 'go', 'going' and 'goes' are all replaced with the word 'go')\n",
    "\n",
    "We first write the function ``prepare_text`` that processes one text message string and returns a list of of strings each string representing a word in the message.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orginal text:\tPlease don't text me anymore. I have nothing else to say.\n",
      "prepared text:\tpleas n't text anymor noth els say "
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "def prepare_text(text, stopwords, stemmer):\n",
    "    text = text.lower()\n",
    "    words = nltk.tokenize.word_tokenize(text)\n",
    "    words = [w for w in words if len(w)>2]\n",
    "    words = [w for w in words if w not in stopwords]\n",
    "    words = [stemmer.stem(w) for w in words]\n",
    "    return words\n",
    "\n",
    "# as an example:\n",
    "k = 100\n",
    "print(\"orginal text:\\t\", data[k][1], sep='')\n",
    "print(\"prepared text:\\t\", end='')\n",
    "for w in prepare_text(data[k][1], stopwords, stemmer):\n",
    "    print(w, end=' ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will join all prepared text strings from all messages, and find the unique words to form the set of words we will work with to represent a message. Note that using a Python ``set`` to keep track of the words in all messages we make sure that all words in the final set are unique. The resulting set is turned into a list again to make indexing simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "allwords = set()\n",
    "for label, text in data:\n",
    "    allwords = allwords.union(set(prepare_text(text, stopwords, stemmer)))\n",
    "    \n",
    "allwords = list(allwords) # make it a list to allow indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set of all words is the dictionary that will be used to represent a message text in a numerical way. For each of the words in the dictionary we will score how many times that word is in the text. We'll make a data array ``X`` such that ``X[i,j]`` is the number of times that the j-th word in the dictionary is in the i-th text. At the same time we make the target vector ``t`` such ``t[i]=1`` in case the i-th text is spam and ``t=0`` for ham. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5cd4ae5810802334",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# first write the function words_to_x that takes a list of words in the text \n",
    "# (i.e. the words in it, so after the text is filtered through the function prepare_text)\n",
    "def words_to_x(words, allwords):\n",
    "    # words: list of words as returned from prepare_text\n",
    "    # allwords: list of all words in the dictionary\n",
    "    #. Your solution here ...\n",
    "    # x: array of integers such that x[j] equals the number\n",
    "    #    of times that word allwords[j] is present in the list words\n",
    "    x = np.zeros(len(allwords), dtype=int)\n",
    "    for i in range(len(allwords)):\n",
    "        # how many times does word occur in words\n",
    "        x[i] = words.count(allwords[i])\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def make_X_y(data, allwords, stopwords, stemmer):\n",
    "    # data: data in the form of a list of tuples (label, text)\n",
    "    # allwords: allwords in entire data set (prepared by te)\n",
    "    m = len(data) # no of texts\n",
    "    n = len(allwords) # no of words in dictionary\n",
    "    X = np.zeros((m,n), dtype=int) # numerical data matrix X[i,j] = no of times word j occurs in text i\n",
    "    y = np.zeros(m, dtype=int) # numerical target vector y[i] = 0 == ham, y[i]=1 == spam\n",
    "    #. Your solution here ...\n",
    "    for i in range(m):\n",
    "        category, text = data[i]\n",
    "        X[i] = words_to_x(prepare_text(text, stopwords, stemmer), allwords)\n",
    "        if category == 'spam':\n",
    "            y[i] += 1\n",
    "        else:\n",
    "            y[i] += 0\n",
    "    # X: data matrix X[i,j] = no of times word j occurs in text i\n",
    "    # y: target vector y[i]=1 for spam and y[i]=0 for ham\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Note: this can take some time...\n",
    "X_total, y_total = make_X_y(data, allwords, stopwords, stemmer)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make an array `word_count` such that `word_count[j]` equals the number of times `allwords[j]` is present *at least once* in all text messages (either spam or not). In calculating this `word_count` array only the `X` array can be used. What is the most frequently used word? And how many times has it been used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f4e4ea972e9254de",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequently used word \"...\" is used 772 times in 8079 messages\n"
     ]
    }
   ],
   "source": [
    "# make the array word_count\n",
    "word_count = np.sum(X_total > 0, 0)\n",
    "j_max_count = np.argmax(word_count)\n",
    "word_max_count = allwords[j_max_count]  \n",
    "print(f'Most frequently used word \"{allwords[j_max_count]}\" is used {word_count[j_max_count]} times in {len(allwords)} messages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[772 590 418 298 281 255 240 239 235 232 228 226 224 221 205 205 200 175\n",
      " 170 170 170 167 166 162 161 158 152 149 149 146 142 140 140 139 135 134\n",
      " 133 132 132 131 130 127 125 124 120 120 115 113 112 112 111 110 109 109\n",
      " 107 105 104 103 101  98  98  97  97  96  96  94  92  91  89  89  88  88\n",
      "  87  87  86  86  86  86  86  84  83  82  82  81  80  80  79  78  76  76\n",
      "  75  75  74  74  74  74  73  72  72  71  71  71  71  70  69  69  68  68\n",
      "  68  67  66  65  65  65  64  63  63  62  62  61  61  61  60  60  60  60\n",
      "  60  59  59  58  58  58  57  57  57  57  57  57  57  55  55  55  55  54\n",
      "  54  53  53  53  52  52  51  51  51  51  50  50  50  50  49  49  49  48\n",
      "  48  48  48  48  47  47  47  46  46  46  45  45  45  44  44  44  44  44\n",
      "  44  44  44  44  43  43  43  42  42  42  42  42  41  41  41  41  41  41\n",
      "  41  40  40  40  40  39  39  39  39  39  39  39  39  38  38  38  38  38\n",
      "  38  38  37  37  37  37  36  36  36  36  36  36  36  36  36  35  35  35\n",
      "  35  35  35  35  35  35  35  35  35  35  34  34  34  34  34  34  33  33\n",
      "  33  33  33  33  33  33  33  33  33  32  32  32  32  32  32  32  31  31\n",
      "  31  31  31  31  31  31  31  30  30  30  30  30  30  30  30  30  30  29\n",
      "  29  29  29  29  29  29  29  29  29  29  29  29  29  29  28  28  28  28\n",
      "  28  28  28  28  27  27  27  27  27  27  27  27  27  26  26  26  26  26\n",
      "  26  26  26  26  26  26  26  26  26  26  26  25  25  25  25  25  25  25\n",
      "  25  25  25  24  24  24  24  24  24  24  24  24  24  24  24  24  24  24\n",
      "  23  23  23  23  23  23  23  23  23  23  23  23  22  22  22  22  22  22\n",
      "  22  22  22  22  22  22  22  22  22  22  22  21  21  21  21  21  21  21\n",
      "  21  21  21  21  21  21  21  21  21  21  21  21  21  21  21  21  20  20\n",
      "  20  20  20  20  20  20  20  20  20  20  20  20  20  20  20  20  19  19\n",
      "  19  19  19  19  19  19  19  19  19  19  19  19  19  19  19  19  19  19\n",
      "  19  19  19  19  19  19  18  18  18  18  18  18  18  18  18  18  18  18\n",
      "  18  18  18  18  18  18  18  18  18  18  17  17  17  17  17  17  17  17\n",
      "  17  17  17  17  17  17  17  17  17  17  17  16  16  16  16  16  16  16\n",
      "  16  16  16  16  16  16  16  16  16  16  16  16  16  16  16  16  16  16\n",
      "  16  16  16  16  16  16  16  16  16  16  16  16  16  16  16  16  16  16\n",
      "  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15  15\n",
      "  15  14  14  14  14  14  14  14  14  14  14  14  14  14  14  14  14  14\n",
      "  14  14  14  14  14  14  14  14  14  14  14  14  14  14  13  13  13  13\n",
      "  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13\n",
      "  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13\n",
      "  13  13  13  13  12  12  12  12  12  12  12  12  12  12  12  12  12  12\n",
      "  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12\n",
      "  12  12  12  12  12  12  12  12  12  12  11  11  11  11  11  11  11  11\n",
      "  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11\n",
      "  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11\n",
      "  11  11  11  11  11  11  11  11  11  11  11  11  10  10  10  10  10  10\n",
      "  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10\n",
      "  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10\n",
      "  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10\n",
      "  10  10  10   9   9   9   9   9   9   9   9   9   9   9   9   9   9   9\n",
      "   9   9   9   9   9   9   9   9   9   9   9   9   9   9   9   9   9   9\n",
      "   9   9   9   9   9   9   9   9   9   9   9   9   9   9   9   9   9   9\n",
      "   9   9   9   9   9   9   9   9   9   9   9   9   9   9   9   9   9   9\n",
      "   9   9   9   9   9   9   8   8   8   8   8   8   8   8   8   8   8   8\n",
      "   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8\n",
      "   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8\n",
      "   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8\n",
      "   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8\n",
      "   8   8   8   8   8   8   8   8   8   7   7   7   7   7   7   7   7   7\n",
      "   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7   7   7   7   7   7   7   7   6   6   6   6   6   6   6   6   6\n",
      "   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6\n",
      "   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6\n",
      "   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6\n",
      "   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6\n",
      "   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6\n",
      "   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6\n",
      "   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6\n",
      "   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6\n",
      "   6   6   6   6   6   6   6   6   6   5   5   5   5   5   5   5   5   5\n",
      "   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5\n",
      "   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5\n",
      "   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5\n",
      "   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5\n",
      "   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5\n",
      "   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5\n",
      "   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5\n",
      "   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5\n",
      "   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5\n",
      "   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5   5\n",
      "   5   5   5   5   5   5   5   5   5   5   5   4   4   4   4   4   4   4\n",
      "   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4\n",
      "   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4\n",
      "   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4\n",
      "   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4\n",
      "   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4\n",
      "   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4\n",
      "   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4\n",
      "   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4\n",
      "   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4\n",
      "   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4\n",
      "   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4\n",
      "   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4\n",
      "   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4\n",
      "   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4\n",
      "   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4\n",
      "   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4\n",
      "   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4\n",
      "   4   4   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
      "   3   3   3   3   3   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1]\n"
     ]
    }
   ],
   "source": [
    "# We'll make the array j_sorted giving the indices of the sorted word_count array (in decreasing order)\n",
    "j_sorted = np.argsort(word_count)[::-1]\n",
    "print(word_count[j_sorted])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the message (and its label) containing the maximal number of the same word? And which word is that? (not an exercise only a demo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the 'ham' message 3017 word 'happi' occurs 15 times, the message is:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "&lt;#&gt;  is fast approaching. So, Wish u a very Happy New Year Happy Sankranti Happy republic day Happy Valentines Day Happy Shivratri Happy Ugadi Happy Fools day Happy May Day Happy Independence Day, Happy Friendship,Mother,Father,Teachers,Childrens Day, &amp; HAPPY BIRTHDAY 4 U. Happy Ganesh festival Happy Dasara Happy Diwali Happy Christmas  &lt;#&gt;  Good Mornings Afternoons, Evenings Nights. RememberI AM the first to WISHING U ALL THESE...your's Raj"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "msg_idxs, word_idxs = np.nonzero(X_total==X_total.max())\n",
    "message_idx_max_freq = msg_idxs[0]\n",
    "word_idx_max_freq = word_idxs[0]\n",
    "print(f\"In the '{data[message_idx_max_freq][0]}' message {message_idx_max_freq} word '{allwords[word_idx_max_freq]}'\", end=\" \")\n",
    "print(f\"occurs {X_total[message_idx_max_freq, word_idx_max_freq]} times, the message is:\")\n",
    "printmd(data[message_idx_max_freq][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making the Train and Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all machine learning applications, it is important to make a distinction between training and test set. The easy way would be to make the training set out of the first examples in your data set and reserve the rest for the test set. But then you run the risk that the first examples might not be a faithful representation of the entire population (remember Trump doing that, interpreting the first poll results as being representative for the entire state? Obviously he didn't follow any course on statistics while he even had 'the best education'). Therefore we shuffle the rows in the data set first (well actually we only shuffle the indices...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-52696a4495a7f6bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m: 5572\n",
      "train_idx [3245  944 1044 2484  812 2973 2991 2942  230 1181 1912 1992 5435 4805\n",
      "  401 1859 1344 2952  501 3337 1945 3142 2422  381 5567 4937   79 5240\n",
      " 2554 5345 4379 4789  683 5519 4315  393 5541 4546 3599 2225  881 4625\n",
      " 3997 5015   23 4479 1215 1961 5339  848 4294 2664 3407  351 3492 3103\n",
      " 3315 1881 3504 2380 1598 4733 3299  439 2903 1186 1728 3975 3050 1128\n",
      " 4500  315  296 3298 3864    8 5003 2968 5194 2157 3669 4730 5349 2388\n",
      " 2575 4085 4328 3467 2245 4269   33 1281 5145 2598 3532 1253 4707 2658\n",
      " 3479 4483 4700 3116 3785 3547 1039 3859 4383 3403 3710 1643 1055 1817\n",
      " 2780  290 2972  319 1467 1335 1419 3548 4388 5156 2462 3494 5209  964\n",
      "  553 2932 2801 5178 3872 2841 1033 2231  809  538 4121 1103  346 3318\n",
      " 4738 1870  696 4160 2536 2650 4808 5022  544 1632 4713 1533 3295 2384\n",
      " 2726 5128 3328 4050 3307 2067 2402 2317 3559 2022 4323 3889 4527   15\n",
      " 5259 3257 2357 3543 4677 3803 4472 3112 2995 3484 4536   84 2487 3703\n",
      " 3966 2927 2101 4253 3535 4402  168 1173  530 4022 2576 3689 1924 4071\n",
      " 1334  838 5210 5558 4352 2347 5493 3820 5549  426 3366 4999  418 4892\n",
      " 3074 5250 2527 2210 4623 2795  373 3660 1199 4427 5498 2312 5125 3933\n",
      " 1049 3979 3455  132  468 5263   80 1219 2488 5415 2663  297 4602 1094\n",
      " 3468  577 5164 1261 1879 3433 4883 2714 1410 3570 3158 1504 5444  167\n",
      " 1837 5406 4920 1557 5204  764 5031  422 1022 4369 3199 1779 1412 4184\n",
      " 1305 3552 4539 2341 5561 5155  751  893 1966 1612 2746  584 1873  279\n",
      "  135 1562 2905 5127 1158 3615 4601 2364 2858 3043 2034 4376 2264 3197\n",
      " 1558  865  747 2480 1586  121  324 4956 1074  925  877 4331  589  333\n",
      "  896 2753 3515 1513 5397 2086 4048 5332 3208 2789 5341 1505 4816 5289\n",
      "  534 2775 3463 2366  240 2805 4418 3886 2110 5174 5507 5114 1370 2059\n",
      " 3655 3247 1942 2877 4588 1934 5142 3873 4652 1694  794 1361 1666 4853\n",
      " 1392  208 3077 1703 2822  248 1580  598 4524 3539 2236 5086 2985 1212\n",
      "  712 1086  453 3728  889 1937  102 4922 1231 4333 2550 3311  107 3414\n",
      "   29 3945 4490 4818 1400  239 5270 2329 2471 3181 2284 5506  737 4228\n",
      " 2721 4410 2495 3562  465 1268 2440 1297  199 4368 1564 4274 5072 5076\n",
      " 3722 2727 2313 3113 3816 2997 3195 3542 1224 4435 2252 5198 5207 3994\n",
      " 4876 3384 3603 4013 2277 4357  763 1760 1520 4280  293  371 5227 2759\n",
      " 1057  977  879 1038  626  157   65 4161  245  996  367 2232 3678 3469\n",
      "  730 5193 2842 3897 1786 1561 3725  599 3958 5026 5038 5307  681 1126\n",
      " 1835 2052 2330 1941  151 2673 1539 5182 4047  447 3405 5167 3793  810\n",
      " 3662 2531 4152 1620 1471  576 2447  711  829 5434  625 5350 1630 3164\n",
      " 4874  410  491 5262 4879 5480 4665  471 1732 3912 2118 3362 1720 2080\n",
      " 3593  425 4908 2195 1877 2760 1957 1665 2732  251 5530 3592 3892 3946\n",
      "  227 4586 3207 1684 2662  605  926  624 2083 4271 3288  292 3885 3846\n",
      " 4537 4074 1491  555 1095 1255 5171 5526 3256  611  994 2053 1888  907\n",
      "  705 1813 2387  655 3755 3014  505 3586 4686 4046 2836 4913 3130  106\n",
      " 4697  856 3454 2095  800 5181 4239 4417 1545 1046 3948 1323 4661  416\n",
      " 1606 4339 4574  478 2369  179 4806 4644 5565 5317 1117 2954  903 4110\n",
      "  497  724 2534  776 3576 1566  472 3612 3194 4861 3321  473 3822 1862\n",
      " 1614 1197 4494 3438  272 4900  336 4147  748 1073 2417 2233 4340 5274\n",
      "  486 2670  286 4585 3017 5111  254 3361 4321 1188 3000 2749 4881 5103\n",
      " 4732 3276 2167 2571 1235 3360 3790 3402 1448 3106 4904 2803 3450 1839\n",
      " 1611 4288 1242 4077 3520 3013 1569  765 4296 2276 5474 3903  228 2996\n",
      " 4916 3712 1752  691   17 4532 4431 1658  443 2970   37 1468 2723 3779\n",
      " 2131 1411 2314 4663 2306 2370 4506 5036 5450 2767 5363 2213 3410 5479\n",
      "   88 5275 4437 4515 3580  787 4971 4428 4122  144 4961 5261 3411 4615\n",
      " 3309 1420 3185 3089  485  734 1831 5081 2497 1822 5255 1121 5543 4351\n",
      " 1032 4292 5488  807 3887 4457 4923 1747  100 3832 5028 2755 5124 3751\n",
      " 1717 3706 5060 5366 5534 1669 1509  109 4520 2463 4684 2072 2406 5037\n",
      " 1650 1047 4347 2570 3429 1432 5344 5239 3393 4182 1397 3153 1738 3571\n",
      " 1075 3813 4734  308  742 5483 3443 3033 1025 4251 2519 4993 3827 2351\n",
      " 1084 1330  233 3425 2477 1319 5465 4651 1244 1421 4371 1617 1002   44\n",
      " 1718  746 5044  438 1615 4246 4854 3262  811 4155 4578 2770  149  586\n",
      " 5222  437 5293   12   93 5283  183 1485  642 4705 2244 1071 1220 2549\n",
      " 3752 2228 1315 5141 4584 2826 1498 2642 1894  653  429 4385 1168 1702\n",
      " 1183 4526 5451  915 1536  259   63 5410 2865 4336 4909 3733 2087 3378\n",
      " 4003 1544 2886 2819  721  693 1209 5230 4955  287 4024 3857 5161 5453\n",
      " 3317 1755 2585  561 2458 5109 1543 4581 5380 1436  969 2615 2951 2273\n",
      " 2594  210   47 3163 2153  911 1743 1176 2187 4609 2263 1115 2834 4051\n",
      " 4355 1101 2879 1773 2846 1672 2646 5388  354 2589 1892  932 4640 2287\n",
      " 5459 2029 5089 1654  414 1393 4708 3214 4618 3626 2124 2328 3334 1844\n",
      " 1223 2445 4129  252 3753  654  350 5423 3740 2201 4378  457 3434  978\n",
      "   90  798 3340 5120 3767   26  952  549 3497 1477  181 1292  263 2835\n",
      " 2321 2640 4841 4459 2561 3200 2654 5149 4211 5054 3128 4796 3095 4845\n",
      " 3632 2742 3711 5177 4591 1526 4108 1988 2486 2518 4250   69 4080 3718\n",
      "  676 5469 5470 1890 2899  203 3575 2669  339 4407 4092 2121  479  387\n",
      " 1020 4343 2133 3891 1729 3227 1350 4004 3082 5502 3094 4951 2603 3758\n",
      " 4878 2100 3995 2436  221 5055  677 1144 1374 1599  805   61 1090 3819\n",
      " 4005 3135 1438 1652 4295 4403 5140  177 4470 2456 5400 4039 5446 4081\n",
      " 5457 4516 2515  733 3217 4709 2677  527 3616 1874 1593 1425 4837  803\n",
      " 4950 1097 2107 2473 4350 1129  219 4608  582 2750   43 1175 5420 3901\n",
      " 4886  718 4010  670 3400 3211 1514 2944   95 1770  842 1321 4666 4629\n",
      " 2481 1962 2963 2348  322 4008 4905 5063  334 2873 3537  833 3080 4810\n",
      " 3529 2465 3880  731  291  184 2377 2641 1407  533 3904 3858 1595 4903\n",
      "  657 4674 3002 5371 2259  898  428 3641 2191 1483 1699 2135 1789 1163\n",
      " 1193 2941 1553 2138 4264 2439 5556 4205 4293 1978 3989 3935 4078 4086\n",
      " 4919 2268 4696 3653   70 4467 2418 2623 1272 1454  831 5082 1893 3269\n",
      "  238 5426 4867 3435 5542 1295 2998 1001 1512 4370 3458 2350  957 2229\n",
      " 4301 2240 1437 5351 1876 2094 2655 3482 1034 4764  931 5087 2902  633\n",
      " 2451  498 4935  415 2698 2855 3727 1426  757 1194 1041 1777 2572 2699\n",
      " 2168  596 1919 2763 2807  652 5151 3024 3271 4113 1406 1618   75 1964\n",
      " 3336 3842 5358 4510 2410 2988 2164 3085 1010 1029 1745  828 1345   96\n",
      " 3807 4279 2977 1200  621  566  543 4100   51 4820 3039  745 3940 1480\n",
      " 5314  195 2559 1302 3297  802 3717 4638 1322 3674 4248 3611 4605 4031\n",
      "  644 1488 4181 1864  120 4486 3226 2024 4482 4329 3929 3656 3023 4857\n",
      " 2815  134 3038 1918 3456 4681 1578 1788  511   62 3724 2890 3372 3213\n",
      " 1973  122 2298 3560 4844 4722 1210  567 3651 3942  864 4224 3893 4443\n",
      " 1741 2618  720 3349 2509  690  602 4755 1756 1782 2569 2170 4190  318\n",
      " 3352  274 4797 4327 4668 4375 1507 1840 4387 5373  295 4306 2694  298\n",
      "   68  736 5523 3109 1550  150  756 1501 3053 2957 2333 2057  376 1499\n",
      "  859 1351 4757 2916  378 3877 1294 1379 2792 2812 4690 3090  841 5377\n",
      " 2042  175 4066  927 3063   45 1260 1096 2706 1902 4739 2535 1030 1869\n",
      " 3836 2197  283 2173 4888 3210 4553 1616 5452 4006 1371 2683 4266 2115\n",
      " 1907 2271 3323 4759 1456 3007 5080  257  507 3105   99 2940 2004 5398\n",
      " 4519 2209 1740 2722 4127 4028 4562  366 5065 5419 4318 3132  949 2450\n",
      "   19 1971  668  218 5536 1657 3875 4281  585 4545 3034 2758  777 2600\n",
      " 1018 2182 2784 5340 3909 1433 2638  517 5348 1983 2378  217 2627 5212\n",
      " 1891 2910 2619 1052 5378  759 3290 3481  214 1600 3326 5224 2611 2127\n",
      " 2098  196 1554 2653 5431 5318 1056 4710  887 2006 4535 3066  568 4043\n",
      " 3162 1487 5237  139 4412 1807 1149 2114 2177 4783  463 2881 2692 4639\n",
      "  594 4458  450 5524 3770 5471 3048 1921 1849 3944 2304 1174 4996  451\n",
      " 4687 3692 3614 4194 3119 1263 5218  999 3282 3745 5219 1476  192 4880\n",
      " 5021 4715 1726 4706 5195 1338 4772 4477 2441 4885 1293 4200 4667 2018\n",
      " 2609 3279 3670  368 5304 1151 4701 2404 2144 3101 5478  191 4850  108\n",
      " 5121 2925 3346 1360 1769 4856 5421 2457 2218 3622 3333  476 3694 4543\n",
      " 3896  312  508  166 3381 4114 5408 1024 1221 3209 1119 3649 5018 2103\n",
      " 2781 5395 5099 4617 4978 3589 1860 1583 3658  152 2251 3126 5569 2478\n",
      "  725 1721 1503 3070 3538 2215 5369 2031 2303   31 1225  937 3554  752\n",
      " 3679  309 4179 2119 3856 3716 2602 4002  495 1405 3759  205 1434 1135\n",
      " 3296  868  701 5383  305 4219 1192 4396 2389 1534 1061  843 3353 2787\n",
      " 1538  211 1340 1108 4346  679 5059 3617  565 4466 1532 4270  680  180\n",
      " 5187 2254 2864 5269  278 1647 5243 1204 1905  432  289 1027  535 2751\n",
      "  993 1187  188 4241 5414 4099 3795  706 2104  270 2574 4552 3049 2412\n",
      " 4760 1832 4332  111 1170 1391 4337  220 1662  627 3545 2809   83 3102\n",
      "  790 2476  461 4907  551 3375 4020 3516  198 3193 2857  402 3782 4456\n",
      " 5308 2505 2525 4975  650 4463  708 4338 5029 4921 2592 2275 1424 1216\n",
      " 5211 4743  314 3915 4434 1427 1780 4988  113  960 4793 2794 1114 4104\n",
      " 5571 3749 3956 4397 4168 5048 1730 4314 3399 1634 3437 1820 3392 2689\n",
      " 4012  326  604 4664 4049 4685 5246 5310 4825 1765 1858 1288 4756 3536\n",
      " 3739 5416 2337  348 2099 3319 2728  789 1626 2868 1871 3573  321  871\n",
      "  169 3921 2517 2552 4817 4374 5069   73  420 2093 2002 3729   92 5096\n",
      " 1623 1897 4404 2776 5157 2318 2309 3919  494 1375 3874 2704 4782 1164\n",
      " 4011 1270 2311 2145 2688 2292  618 4776 2680  124 1886 2981 2223 3661\n",
      " 3431 5085 4953 4166  869 3358 1042 2025 3664  356 1162 3926 3418 3341\n",
      " 2874 2862 2181 5162 4189  579 4304 2771 2224 4098 1691 1867 4060 4994\n",
      "  332 1926 2894 1744 3383 3792 1258 4319 1451 2399 1116 1541 3618 4252\n",
      " 1954 5329 3045 4041 3098 2372  445 2778 4619 4165 1413  452 5375 2885\n",
      " 4944 3313 3652   14 4758 3235  945 5319 5528  247 3986  360 5347 3781\n",
      " 5188 1776 3878 4169 2847 4259 3922 2696 1592 1723 2299 5183 5217 5441\n",
      " 1965 3527 5264  490 4176  862 1006 4255 1383 2405 3574 4866 2235 1362\n",
      " 4125  506 1417 3303   25 5359 3388 1222 3815 1113 2656 1552 4769 5107\n",
      " 2580 3583 5208 3668  433 5115 3906 4649 3133 3503 2577 5425 2856 3916\n",
      " 2629 4172 3604 1102 2117  411 3493 4424  156 3686  194  615  643 4936\n",
      " 5553 1068 4590 5292 4503 3259 4763 4568  330 2302 3260 3047  622 3096\n",
      " 1670 2876  483 3588 2687 3528 2045 5078 3837 4873  755  518 4502   32\n",
      " 4416 4391 3513 3123 1157 1444 2091  744 5132  174 5432 1705 3161 2063\n",
      " 1818 3835 5214  103 3196 4140  783 3470 4237 4815 4356  229  857 1211\n",
      " 1401 2917  408  170  540 1559 5286 4989 1803 3168  613 4349 3862 4794\n",
      "  674 1161 5532 3939 1872 2526 1815 2850 1700 3204 2226 1609 2342 3121\n",
      " 2718  564 4702 4480 4727 4162 2582 1548 4802 4400 5491 5288  355 2409\n",
      " 2346 2955 4630 4940 1172 3871  325 1051 3639 4075 3100 4382 3154 1751\n",
      " 2499 2146 1772 1196 1878  620 2887 2016 2005 3558 5043 1482 1189  328\n",
      "  648 3648  897   30 1123 2414 4489 3613 2432 4671 1259 4653 2974 2344\n",
      " 1519 1378 4054 1023 2503 3861 2077  912 3524 1675 1746 2584 1736 1629\n",
      " 3320  430 1808 3075 5095 5073 3035 3011 3690 3248 4974 3176 4952 1995\n",
      " 2764 1106  818 3555 5234 1697 3802  136 2818 2543 5046 1180 3907  888\n",
      " 2227 3250  817 4055 4128 5131 1264  990 3091 1739 4766  353 4254 2142\n",
      " 5201 1489 3428 1228 3866 5281   52 3734 3447 2895 3182 1589 1479 1469\n",
      " 2416 2078 3563 5487 3010  436 2513 1588 4367 1590 2829  973 1238 2154\n",
      " 3129 4917 1497  449 4513 5009 2607 1551 4948 1146 3258 2092 1313 2015\n",
      " 2845 2373 2617 3732 2827  413  941  801 4411  256 5464  647 1767 4773\n",
      " 4130 2596 2288   24 1088 4807 2291  112 2407   56 1309  695 1227 3705\n",
      "  276 2316 1833  682 4589 3980 1137 3685 2620 2986 1346 2222 5260 2437\n",
      " 3426 3787 3243 4034 2833 3169 2464 5268 4582   81 1972 1714 5110 4348\n",
      " 1946 4838 3764 1610 2820 2516 4693 5443 4302  665 2194 3252 4746 1017\n",
      "  557 3305 2962 1207 3143 2633 1510 4377 5041  266 2339 2921 2111 3404\n",
      " 1909 2700 3408 3809 3478 4786 1784 1237 4645 5496 2186 4595  908 1303\n",
      " 4214 1627 1522  835  358  138  299 1556 1003  382 4826  761 3145 5477\n",
      " 3413 4509 1402 4622 4790 1932 2522 4445 4768 2459 3379   58   76  796\n",
      " 4450 4037  785 2523 4604 2217 3698 5153 4119 2924 4305  910 2172 4414\n",
      " 5221 1257 4765 4373 1178 4201 5285  442 3550 2980 4740 5315 1054 2323\n",
      " 4326 5244 4059 4753 2586 2420 4056 1502 4749 4781 2011 4229 2671 4498\n",
      " 4216    6  678 1943 3240 5133 4461 4284 3107 5130 1064 3254 2242 1474\n",
      " 4839 1889 4153 2021 4390 4196 3268 1130 1461 2741 5049 2882 2851  222\n",
      " 5170   48  573 1236 4245 5402 5012 2282  267 1104 2305 5368 1935  386\n",
      " 4093 3825 5047 4882 1563 4540  209  909 1289 4213 3625 1842 4561 4260\n",
      "  958 1793 2088  331 2908 4354 2605 1774 3544 4023 4528 2498 3286 3477\n",
      " 1710 4262 1366  173 4320 4109 2521 4863 1952 2926 2036 3386 4082 4688\n",
      "    0 2174 1356 4154 2765 2748  396 4195 2915 1812 3148 5236  816 1572\n",
      " 3667 1800 2280 5407 4220 2859 4812 3370 3008 3774 3742 3918  554 1159\n",
      " 1213 4899 1325 2860 2922  700 3908 4576 4096 1455 4669 3114 5192  866\n",
      " 1644 3931 4689 3149 1490 2685  729 3357 5189 1936 3509 1110 4531 4984\n",
      " 2394 1185 2007  193 1515  578 2123 1429  771 2395 5516 1846  131  178\n",
      " 4979 3489 4215 3159 4726 3480 2073  658 1896 4101 2221 5277 2379 3839\n",
      "  342 5042  727 4393  976 5357 3684 1928 3221 4009 1299 4454 5122  660\n",
      " 1790 3565 3442 3680 2707 3830 2705 3646 5135 1085 3951 1278 3799 1621\n",
      " 4741 5105   67 3184 2622 2269 5070 4395  820 1226 3704 4662 2367 2647\n",
      " 4675 2512 4969 4750 4307 1143  397 4945 4785 3412 1398 1659 2220  979\n",
      " 2331 1298 2802   82 3460 3285 4359 4392 3621  423 4829 1904 5058 1530\n",
      " 3841 1565 4126  839  500  141 1377 4334 5460 2266 2644 1138 2257 3139\n",
      " 1787 3167  550 3838 2019  948 3585 2715  532 5481 1829 3606 4925 4442\n",
      " 2943 1602 2238  639 5382  967 4714 5220 1683 2248 3634 4983 2893  155\n",
      " 2295 3071 4560 1089 3270 2319 4872  288  930 3127 2945 4267 5062 4139\n",
      " 5000 4072  942 2354 4803 4813 5197 1698 5247 3654 5566 3473 3796  985\n",
      " 3957 2702 2754 5322  981 3076 5074 3001 3914 3635 4235 2929 5563 1457\n",
      " 1395  264 3881  929 3789 2937 2400 2178 3322  246 1594 1758 5418 4420\n",
      "  923 5066  268  669 3382 1910 4475 1152 5364 4824 2591 4801 1781 2413\n",
      " 2307 2678 3691 1915  612 4419 2267 2533   39 1898 1649 3026  258 2434\n",
      "  847 4084 4731 3683 1233 1127 3044 1967  558  904 2614  484 2211 1320\n",
      " 5439 2555    7 2315 4725 4058 3949 1450 1201 2426 2648 1814  773 2274\n",
      " 1011 3483 3624 1991 1277 1783 4265  446  880 4972  781 2651 5412 1749\n",
      " 2126 4603 2423 5437 4898  883 4342 1091 3988 3630  531 3519 2438 4522\n",
      " 4105 3682 4566 2290 1727  610  570 3631 3750  343 3715 4914  434 1903\n",
      "   97 4831 2069 1537 2788 4439 2897 1517  965 5294  528  176 2470 4207\n",
      " 3137 4894  572 2843  592 4628 5476 2374 5229 3978 4453 2208 3166 2492\n",
      " 3406  383  852 1880  370 4103 3801 4811 1105 4025 2361 4484 1336 2189\n",
      " 5440 5025 2867 2246 3619 5399 1171 3976 4930 1422 2686 2892 3265 4717\n",
      " 1464  874 2297 2643 4504 4830 1251 2900  840 4438  581  204 1641 2014\n",
      "  905  631 3828 1974  162 3927 3540 5011 3062  617  875 1431 4365  547\n",
      " 4112 5067 4698 2948 4965 5139 5138 1861 2659 5119  640 3697 5442  637\n",
      " 4556 4192 2308 5389 2096 5333 3806 1372 5374  921  546 1449 2804 2610\n",
      " 1900 2757 2028 2301 1885 1642  972 5356 2097 4848 4258 5010 5533  836\n",
      "  601 2184 4436 3065 4792 1582 4747 3125 4577 1070 1775 5355 1283 5303\n",
      " 4053 5297 1836 5235 1019 3971 1326 3064  475 2810 4183 1651 5176 4767\n",
      " 2528 3707 5361 3187 1373  739 4512 3012  867 1140  997 1337  541 1099\n",
      " 1132 5403 3355 2258 4505   59 4273 1938 4440 5499 2960 2813 5033 1078\n",
      " 4217 3607 3870 4745 2630 3708  940 1080  187 3246 2085 2249 5392 5448\n",
      " 2023 4840 1492 4283  226 4087 2583 1929 3783 5005 1404 2179  697 1124\n",
      " 5391 3608 1778 2634 4579 1037 4647 2256 5504  313 2649 2281 3220 3895\n",
      " 2547 1416 1036 2159 2984 4635 1754 3855 3475 3331 2520 3973 4384 4897\n",
      " 1506 1923  448 4927 5143 5535 3165  361 2183 1134 4447 2155 1830 1053\n",
      " 2460   22 2066 1653 4163 3977 2743 3459 4884 2468 2325  460 1067   87\n",
      " 1430 5199  282 2483  462  212 2588 3206 2907 2581 1368  234 3424  597\n",
      " 4928 2250 4864 2283  662 5529 2285 5256 3955 4310 3844 2990 4960  779\n",
      " 2544 4144 4173 5298 3961 4587 5385  685 5312 3395 5007 2837 3032 2171\n",
      " 3339 1850  487 1688 2230 4040 2320  782  521  522 3505 2939 5447 2398\n",
      " 2207 4204  780 4358 3487 1511 1882  651 5282 3824  369 4871 3253  118\n",
      "  377 3761 2216 4476 1613 2883 3601  962 5020 2510 5231 4247 3488  359\n",
      " 3757 2415 5185 4712 2185 4958 2567 2037 4742  772 1284 5126 5278 2587\n",
      " 5097 1414 1234 3332  435 1674  365 3263  231 5159  147 4902 3088 4317\n",
      "  407 2090   86  844 1031 5436 3967 3365 2987 4001 2628 3274  855 4372\n",
      "   78 2992 2247 3541 5091 5129  758 4550 4657 2279 3203 3831 4631 4441\n",
      " 4967 2176  424  392 5266 2239  104 2672 3699 3398 3882 3471 1948 1459\n",
      " 4032 5075 5158 5336 1587 4313 1731 5309 4855  182 1791 2355 1940  702\n",
      " 2442 4573  496 4335  464 3316 4823  172 3954 5343 2540 4038 3647 2260\n",
      " 3084 2112  900 1446 1911 2697 4563  741 2293 2452 3693 4554 1206  509\n",
      " 4626 5445 3464 5050 1549 3029 4044 2202 2566 3306 2044  767 5427 1508\n",
      " 3134  421  123 4634 4209  819 1403 1382 2148 4694 1575 1638 1795 5362\n",
      " 4963  399 3151 2310  882 5539 4957 1352 3390 5071 2130  982  456 1486\n",
      " 1079 1950 4800  689 2429  641 2392 2825 1953 3396 3058 5257 4180 2710\n",
      " 1083 4137 5554 3620 1884 4198  389 2494 4076 3036 4462  845  542   91\n",
      " 5242 3409 1979 3244 5381 3150 4557 3590 4017 2964 2529 3605 1676 1048\n",
      " 2949 1960 2530 4102 3439  237 3666 4210 1133 2880 1805 4906  163 4559\n",
      " 5302 5475 4118 4998 3964  265 2001 1811 3277 2978 3852 2906 3898 1428\n",
      "  398 3982  947 3078 3138 5305 5180 4791 3229 3721 1452  593 2725   41\n",
      "  560 5296 3628 3768  243 3172 1339 5328 3659  405 1442  261 4572 5084\n",
      " 1689 1725 1203  834 1866 2496 3016 5313 2010 3821  340 3700 1601 2796\n",
      " 3720 5396  480 3491 2785 1523 4821 4949 5168 3773  799 1766 4987 3970\n",
      " 2453 5531 4497 4517 5213  363 1271 4309 5017 3829 5503 4624  347 3671\n",
      " 4448 5150 2562 3027 5112 1423  656 3968 5137 2102  632 3578 2493 2665\n",
      " 3984 4316 4091 3834 3833 4275 2500 1465 3501 4508  115 1821 1494 2401\n",
      " 1232 2661 2817 3551 5094 1153 5316 4954   57 4157 5098 4770 5509 1735\n",
      " 5023 2823 2196  159 2475 2564 3996 3657 2608 4311  707 3046 4761 1093\n",
      " 4257 1376 5473   72 1922 3928 1764  743 3557 1005 2668 1190 3950 2601\n",
      " 1125 1241 3523 4600 5252 3351  998 4399 3518 2684 4834 1577 5337 1826\n",
      " 4073 5557 4299 4606  440 5515 1100 2637 3786  649 1546 3231 4240 4256\n",
      " 3917 5508 4361  630  672 1249 3567  482 1584 3805 5154 3765 1462 3040\n",
      " 4473 4290 5547 3273 3238 3131 2578 4398   49 4771 3784  858 3225 1072\n",
      "  719 3910 2631 5428 1660 1656 2425 4233 4704 5540 4094 3097 4990 4088\n",
      " 5295 3189 2934 4596 3848 2606 1385 4421 1399 1195 1408 1845 1798 1475\n",
      " 3985   35 4525 2345 3932 3312 2359 3006 2206 5384 4778 3179 3422 1286\n",
      " 4995  349 3416 2137 2691 3905 3868 1681 4521 1443 4016 3215 1671 3650\n",
      " 1179 2798 1692 4444  372 3055 4286 1916 1357 1825  786  469  140  788\n",
      "  323   50  714 5004 2840 2446 5360 4089 4926 5118 5202 2666  406   20\n",
      " 1004 5331  890 2411 2545 1349 2390 4481   42 2830 4095 3371 3022 3364\n",
      "  303 5030   11  664 1177 3292 2676 1947 2501 1473 2863 2375 2132 1026\n",
      " 2989  961 2889 3021 3015 3879 3338 2009 2149 1963 2338 3800  735  304\n",
      " 1440 1496 2652 2875  916 4655  710 5152 4918 2772 1624 4423  821 4083\n",
      " 1354 5013  850 3350 1000 5101 3947 4148 3525   18 1637 1667  493 3173\n",
      " 5468 4244 1347 3018 4353 3230 5413 4287 2828  687 1112 3232 1608 4558\n",
      "  703 1009 3291 3673 2408 1092 1327 5190 1677 1999 1310 3960 3780 4449\n",
      " 1987 4308  344 5394  986  587 2791 5500 4149 3849 4511 3485 3826 3754\n",
      " 3289 1142 2358 3264 1198   85 1518  516 1381 3941 2716 3817 1014  128\n",
      " 1364 4291 5321 3867  891 2793 4683 1359  307 1661 2431 4478 1317 3499\n",
      " 4787 3249 2474 2542 5372 1246 4643 1208 3377  216 4775 5300 1239 2289\n",
      " 2740  275  489 1035 1229 5106 1008 5245 3865 3938 2532 4641 1058 2166\n",
      " 4567 4141  281 1043 1316 5544  968 4819 3869 1939 4394 1274  619 4762\n",
      " 1742 1390 4541 1847 1579 1655 3568 2352 2212 3275 2020 5538 1956 3178\n",
      " 3192 1307 2003 4966  575 2360 4676 4947 1706 1799 3347 4991 1280 4538\n",
      "  861 4616 1801 1628 1993 2639  269  629 4231 3081 3466 2334 4870 2966\n",
      "  300 4159  459 1447 1472 2046 4703 4381 5301 2113   27 2040 5040 3216\n",
      "  837 3814  129 4938 2008 5466  503 4230 4788  380 1851 1525 2365 4030\n",
      " 3052 2884 2340  374 4035 2381 4795  939 5338 4430 4852 2799 2147 3591\n",
      " 4610 5061 3808 4691 2068 3623  529 3308 3937 5527  636 1269  185 1394\n",
      " 3934 4187 1797 2979 1441 4174 5429 3811]\n",
      "len train_idx: 3900\n",
      "X train: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "y train:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 1\n",
      " 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0\n",
      " 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 1 1 1 0 0\n",
      " 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0\n",
      " 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1\n",
      " 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0\n",
      " 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0\n",
      " 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1\n",
      " 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1\n",
      " 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0\n",
      " 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1\n",
      " 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n",
      " 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
      " 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0\n",
      " 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1\n",
      " 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1\n",
      " 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0\n",
      " 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
      " 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1\n",
      " 0 0 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "m = len(data)\n",
    "random_idx = np.arange(m)\n",
    "np.random.shuffle(random_idx) # note that shuffling is done 'inline'\n",
    "m_train = int(0.7*m) # we reserve 70% for learning\n",
    "m_test = m - m_train # and 30% for testing\n",
    "train_idx = random_idx[:m_train]\n",
    "X_train = X_total[train_idx,:]\n",
    "y_train = y_total[train_idx]\n",
    "test_idx = random_idx[m_train:]\n",
    "X_test = X_total[test_idx]\n",
    "y_test = y_total[test_idx]\n",
    "print(\"m:\", m)\n",
    "print(\"train_idx\", train_idx)\n",
    "print(\"len train_idx:\", len(train_idx))\n",
    "print(\"X train:\",X_train)\n",
    "print(\"y train: \",y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that by definition every word from the ```allwords``` list are represented in the total set of messages. But that is NOT true for the training set (see code in the cell below). **Please be aware that this fact has consequences on how to estimate the class conditional word probabilities.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(X_total.sum(axis=0).min())\n",
    "print(X_train.sum(axis=0).min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Estimating the Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data matrix $X$ and target vector $y$ we now have to calculate the a priori probabilities\n",
    "$P(Y=y)$ for $y=0$ (ham) and $y=1$ (spam) and the conditional word probabilities $P(W_j=w|Y=y)$ where $W_j=1$ if the j-th word from the array ``allwords`` is present in a message and $W_j=0$ if it isn't present. **Please note that we will not make use of the fact that a word might be present more than once in a message.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will encode the a priori probabilities in the 2 element array `Py` where `Py[j]` is $P(Y=j)$. The conditional word probabilities are encoded in the two dimensional array `Pwgy` such that `Pwgy[i,j]` is equal to the probability $P(W_i=1|Y=j)$. In the code cell below you have to estimate all these probabilities. **Please refer to the section on [Estimators for Distribution Parameters](https://staff.fnwi.uva.nl/r.vandenboomgaard/MachineLearning/LectureNotes/ProbabilityStatistics/probEstimation.html).** (Implement the estimator using the \"add 1 Laplace\" smoothing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3e000fedca5b5c3f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00059172 0.0019084 ]\n",
      " [0.00029586 0.0019084 ]\n",
      " [0.00059172 0.0019084 ]\n",
      " ...\n",
      " [0.00088757 0.0019084 ]\n",
      " [0.00029586 0.00381679]\n",
      " [0.00029586 0.0019084 ]]\n"
     ]
    }
   ],
   "source": [
    "# Write the code to calculate the ``Py`` and ``Pwgy`` arrays\n",
    "Py = np.zeros(2)\n",
    "np.set_printoptions(threshold=10000)\n",
    "\n",
    "# print(X_train)\n",
    "# print(y_train)\n",
    "Py[1] = np.sum(y_train)/len(y_train)\n",
    "Py[0] = 1 - Py[1]\n",
    "\n",
    "# Pgwy[i, j] = amount of times word j occurs at least ones in message i + 1 | amount of spam messages + 2.\n",
    "\n",
    "# Pwgy = np.zeros((len(X_train[0]), 2))\n",
    "# print(Pwgy)\n",
    "# for i in range(len(X_train[0])):\n",
    "#     Pwgy[i, 0] = (X_train[:, i][y_train == 0].sum() + 1) / (np.sum(y_train) + 2)\n",
    "#     Pwgy[i, 1] = (X_train[:, i][y_train == 1].sum() + 1) / (len(y_train) - np.sum(y_train) + 2)\n",
    "\n",
    "Pwgy = np.zeros((len(X_train[0]), 2))\n",
    "for i in range(len(X_train[0])):\n",
    "    Pwgy[i, 0] = (X_train[:, i][y_train == 0].astype(bool).sum() + 1) / (len(y_train) - np.sum(y_train) + 2)\n",
    "    Pwgy[i, 1] = (X_train[:, i][y_train == 1].astype(bool).sum() + 1) / (np.sum(y_train) + 2)\n",
    "\n",
    "print(Pwgy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the estimated probabilities, let's see the 10 most probable spam and ham words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent spam words\n",
      "\tProbability\tWord\n",
      "--------------------------------------\n",
      "0\t0.45038\t\tcall\n",
      "1\t0.21947\t\ttxt\n",
      "2\t0.21374\t\tfree\n",
      "3\t0.16985\t\tclaim\n",
      "4\t0.15840\t\tmobil\n",
      "5\t0.15649\t\ttext\n",
      "6\t0.12977\t\tprize\n",
      "7\t0.12023\t\tstop\n",
      "8\t0.12023\t\trepli\n",
      "9\t0.11260\t\tget\n",
      "\n",
      "Most frequent ham words\n",
      "\tProbability\tWord\n",
      "--------------------------------------\n",
      "0\t0.15503\t\t...\n",
      "1\t0.07189\t\tget\n",
      "2\t0.05769\t\tn't\n",
      "3\t0.05621\t\tcome\n",
      "4\t0.05414\t\tcall\n",
      "5\t0.05030\t\tgot\n",
      "6\t0.04763\t\tknow\n",
      "7\t0.04704\t\tlike\n",
      "8\t0.04615\t\t'll\n",
      "9\t0.04379\t\ttime\n"
     ]
    }
   ],
   "source": [
    "idx_Pwg1 = np.argsort(Pwgy[:,1])[::-1]\n",
    "print(\"Most frequent spam words\")\n",
    "print(\"\\tProbability\\tWord\")\n",
    "print(\"--------------------------------------\")\n",
    "for i in range(10):\n",
    "    print(f'{i}\\t{Pwgy[idx_Pwg1[i],1]:#1.5f}\\t\\t{allwords[idx_Pwg1[i]]}')\n",
    "print(\"\")\n",
    "idx_Pwg0 = np.argsort(Pwgy[:,0])[::-1]\n",
    "print(\"Most frequent ham words\")\n",
    "print(\"\\tProbability\\tWord\")\n",
    "print(\"--------------------------------------\")\n",
    "for i in range(10):\n",
    "    print(f'{i}\\t{Pwgy[idx_Pwg0[i],0]:#1.5f}\\t\\t{allwords[idx_Pwg0[i]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell below write the code for the ``spam_classifier`` function. The input is a vector (array) ``x`` where ``x[j]`` is the number of times word ``j`` from the dictionary occurs in a text. Argument ``Pwgy`` encodes all the class conditional probabilities and ``Py`` the a priori probabilities. The function should return a tuple ``(y, Pygt)`` where ``Pygt`` equals $P(Y=y\\bigm|W_1=w_1,\\ldots,W_n=w_n)$ for the value of $y$ that maximizes this probability. **Please note that ``x[i]`` equals the number of occurrences of the i-th word in the dictionary, whereas $w_i$ is 1 in case the i-th word occurs at least once in the message.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dae327cdb81c15f5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def spam_classifier(x, Pwgy, Py):\n",
    "    Pygt = np.zeros(2, dtype=float)\n",
    "    x = np.array(x)\n",
    "\n",
    "    Pygt[0] = np.log(Py[0]) + np.sum(np.log(Pwgy[x > 0, 0])) + np.sum(np.log(1 - Pwgy[x == 0, 0]))\n",
    "    Pygt[1] = np.log(Py[1]) + np.sum(np.log(Pwgy[x > 0, 1])) + np.sum(np.log(1 - Pwgy[x == 0, 1]))\n",
    "\n",
    "    Pygt = np.exp(Pygt - np.max(Pygt))\n",
    "    Pygt /= np.sum(Pygt)\n",
    "\n",
    "    y = np.argmax(Pygt)\n",
    "    return (y, Pygt[y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the next cell allows you to test your classifier on a random text from the entire data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham', \"Trust me. Even if isn't there, its there.\"]\n",
      "0 0.9999999999786102\n"
     ]
    }
   ],
   "source": [
    "k = np.random.randint(0, len(data))\n",
    "print(data[k])\n",
    "y, p = spam_classifier(words_to_x(prepare_text(data[k][1], stopwords, stemmer), allwords), Pwgy, Py)\n",
    "print(y, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b0a89be8a58c7f88",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# The assert statement in this cell uses your classifier on 10 examples.\n",
    "# The numeric array in the assert statement is what the reference implementation returns on \\\n",
    "# the given set of examples (don't change the seed...)\n",
    "np.random.seed(42**2)\n",
    "N = 10\n",
    "ks = np.random.randint(0, len(data), size=N)\n",
    "results = np.zeros((N,2))\n",
    "for i, k in enumerate(ks):\n",
    "    y, p = spam_classifier(words_to_x(prepare_text(data[k][1], stopwords, stemmer), allwords), Pwgy, Py)\n",
    "    results[i] = (y, p)\n",
    "    \n",
    "assert np.allclose(results, np.array([[0.        , 1.        ],\n",
    "       [0.        , 1.        ],\n",
    "       [0.        , 1.        ],\n",
    "       [0.        , 1.        ],\n",
    "       [0.        , 1.        ],\n",
    "       [0.        , 1.        ],\n",
    "       [0.        , 1.        ],\n",
    "       [1.        , 0.99999949],\n",
    "       [0.        , 1.        ],\n",
    "       [0.        , 1.        ]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next run your classifier over the entire test set and calculate the accuracy of the classifier as the percentage of correctly classified examples from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6aebd118bdf4930a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  96.77033492822966\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(X_test, y_test, classifier, Pwgy, Py):\n",
    "    # the function should return the accurracy as a percentage\n",
    "    corr_guess = 0\n",
    "    for i in range(len(X_test)):\n",
    "        y_result = classifier(X_test[i], Pwgy, Py)[0]\n",
    "        if y_result == y_test[i]:\n",
    "            corr_guess += 1\n",
    "    return corr_guess / len(X_test) * 100\n",
    "\n",
    "print('Accuracy = ', calculate_accuracy(X_test, y_test, spam_classifier, Pwgy, Py))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy of 96% is quite good for such a simple classifier. In todays practice spam classifiers need to be a lot better. In general, spam emails make out for more than 90% of all emails and in such cases way too much spam would not be detected and more seriously, way too much ham would be classified as spam and never show up in your mailbox.\n",
    "To improve performance one can \n",
    "- tweak the parameters of the classifier being used,\n",
    "- use another better classifier,\n",
    "- consider more and other features to characterize a text message, or\n",
    "- use a larger dataset to learn from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bayes classifier based on a zero-one loss function classifies a person with given $L$, $W$ and $S$ as $G=0$ (male), $G=1$ (female) or $G=2$ (X or other):\n",
    "$$\\text{classify}(l, w, s) = \\arg\\max_g P(G=g\\bigm| L=l, W=w, S=s)$$\n",
    "The three measurements are real valued and assumed to be values from independent class conditional normal distributions.\n",
    "\n",
    "In the homework exercise you have rewritten the classifier into a form in which the class conditional probability density functions $f_{L|G=g}$, $f_{W|G=g}$ and $f_{S|G=g}$ and the a priori class probabilities $P(G=g)$ are used. Below give this expression for the **naive Bayes classifier**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "bayes_classifier",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "\\begin{equation*}\n",
    "    \\text{classify}(l, w, s) = \\arg\\max_g f_{L|G=g}(l) f_{W|G=g}(w) f_{S|G=g}(s) P(G=g)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this application we assume that the class conditional feature values are normally distributed. When the data was collected in a small group of students only the genders $G=0$ and $G=1$ were present and therefore we will make this application a two class problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning and Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this course we assume that data needed for your programs (ipython notebooks) is stored in files in the the same directory as the notebook. It is important that you adhere to this convention. We also have the data and so you don't have to submit it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you print the head of the 'biometrie2014.csv' file you will see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biometrie Statistisch Redeneren 2013-2014,,,\n",
      "\n",
      ",,,\n",
      "\n",
      "Man/Vrouw,Gewicht,Lengte,Schoenmaat\n",
      "\n",
      ",,,\n",
      "\n",
      "M ,60,183,42\n",
      "\n",
      "M ,130,185,45\n",
      "\n",
      "M ,84,180,42\n",
      "\n",
      "M ,65,195,45\n",
      "\n",
      "M ,55,170,42\n",
      "\n",
      "F,63,173,41\n",
      "\n",
      "M ,74,185,43\n",
      "\n",
      "F,50,170,38\n",
      "\n",
      "F,82,180,42\n",
      "\n",
      "M ,77,190,44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fp = open('biometrie2014.csv') \n",
    "for i in range(14):\n",
    "    print(fp.readline())\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "showing that the first 4 lines are comments, followed by several lines of text where each line contains 4 values separated by comma's (hence the name .csv: comma separated values). Of course you could write your own function to read the file and store the relevant information in data structures of your choice. But be aware that Python 'comes with batteries included.' Most often some task that you need to do is hidden somewhere in the libraries that are standard for python. In this case we will use the 'loadtxt' function from numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numerical work it is advantageous to have all of our data in a numerical (floating point) format. In that case we can store it in one homogeneous data array (an 'ndarray' from Numpy). The first column of data in the file is a character string preventing to load it as a homogeneous array. Therefore we define a converter function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converter0(x):\n",
    "    if x[0]==ord('M'):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and use this to read in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bio = np.loadtxt('biometrie2014.csv', skiprows=4, delimiter=',', converters={0: converter0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.  60. 183.  42.]\n",
      " [  0. 130. 185.  45.]\n",
      " [  0.  84. 180.  42.]\n",
      " [  0.  65. 195.  45.]\n",
      " [  0.  55. 170.  42.]\n",
      " [  1.  63. 173.  41.]\n",
      " [  0.  74. 185.  43.]\n",
      " [  1.  50. 170.  38.]\n",
      " [  1.  82. 180.  42.]\n",
      " [  0.  77. 190.  44.]]\n"
     ]
    }
   ],
   "source": [
    "print(data_bio[:10]) # printing the first 10 ows from the matrix, corresponding with the X, W, L, S values of \n",
    "                 # the first 10 persons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning it is customary to separate the data into the feature matrix (data matrix) X and target vector y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bio = data_bio[:,1:]\n",
    "y_bio = data_bio[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our dataset is so small, we will use the entire dataset for both learning and testing. That is normally not allowed - but to get a first impression it is ok. As a bonus exercise you are asked (at the end) to remedy this error using a cross validation method for testing your classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the Class Conditional Probability Density Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the terms in the naive Bayes classifier you need to calculate the probability densities for weight, length, and shoesize conditionally on the sexe, either female $X=1$ or male $X=0$. We will assume that each of these distributions is a normal distribution (for estimating pdf's as a histogram we need much more data). For each of these six normal pdf's you therefore estimate the mean (expectation) and standard deviation. It turns out that the sample mean and sample standard deviation can be used as estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "estimate_parameters",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def estimate_parameters(X,y):\n",
    "    # data: datamatrix with columns: gender (1 or 0), weight, length, shoesize\n",
    "    # this function should return a tuple\n",
    "    #   (meansF, stdsF, meansM, mstdsM)\n",
    "    # where meansF is (3,) array of the means of females (of weight, length and shoesize)\n",
    "    # and stdsF is (3,) array of the standard deviations\n",
    "    # meansM and stdsM are for the males\n",
    "    meansF = np.zeros(3)\n",
    "    stdsF = np.zeros(3)\n",
    "    meansM = np.zeros(3)\n",
    "    stdsM = np.zeros(3)\n",
    "\n",
    "    for i in range(3):\n",
    "        meansF[i] = np.mean(X[y==1,i])\n",
    "        stdsF[i] = np.std(X[y==1,i])\n",
    "        meansM[i] = np.mean(X[y==0,i])\n",
    "        stdsM[i] = np.std(X[y==0,i])\n",
    "\n",
    "    return (meansF, stdsF, meansM, stdsM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_pars = np.array(estimate_parameters(X_bio, y_bio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make three plots of the pdf's for weight, length and shoesize and in each figure plot the pdf for both genders. For this you will need the ``fNormal`` function from a previous LabExercise. *The plots must be presented in the ANS homework for easy grading. This part of the **Lab** will not be graded.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-7bd94f01c4e43556",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# skipped this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is your task to complete the following function (**remember that for this data set you should NOT take the a priori probability of the two classes into consideration**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "naive_bayes_classifier_code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def fStdNormal(x):\n",
    "    return 1/np.sqrt(2*np.pi) * np.exp( -x**2 / 2 )\n",
    "\n",
    "def fNormal(x, mu, sigma):\n",
    "    return fStdNormal((x-mu)/sigma) / sigma\n",
    "\n",
    "def naive_bayes_classifier(w, l, s, meansF, stdsF, meansM, stdsM):\n",
    "    \"\"\"Return 1 in case the classifier decides the person with \n",
    "    weight=w, length=l and shoesize=s is a female, 0 otherwise\"\"\"\n",
    "\n",
    "    pF = np.prod([fNormal(w, meansF[0], stdsF[0]), fNormal(l, meansF[1], stdsF[1]), fNormal(s, meansF[2], stdsF[2])])\n",
    "    pM = np.prod([fNormal(w, meansM[0], stdsM[0]),fNormal(l, meansM[1], stdsM[1]), fNormal(s, meansM[2], stdsM[2])])\n",
    "    return int(pF > pM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice Python trick is to unravel an iterable (tuple, list, array) into an argument list. Consider the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male\n"
     ]
    }
   ],
   "source": [
    "p = estimate_parameters(X_bio, y_bio)\n",
    "genders = ['male', 'female']\n",
    "print(genders[naive_bayes_classifier(80, 180, 46, *p)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, p is a tuple of all the values returned by estimateParameters, and with *p these values are unravelled into the last 4 arguments for the naiveBayesClassifier function. Be sure that the order of the variables is the same in both cases... In the code below this 'trick' is used twice:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For once we use the learning set to test the classifier. ***Really only this time!*** Feed your classifier with all (w,l,s) triples for all persons in the data set and test whether the outcome of the classifier corresponds with the true value. Present you results in the form of a confusion matrix. \n",
    "\n",
    "<img src=\"https://staff.fnwi.uva.nl/r.vandenboomgaard/downloads/confusionmatrix.png\" width=50%>\n",
    "\n",
    "So it is your task to calculate the 4 entries in the confusion matrix\n",
    "\n",
    "     cfm = array( [[MasM, FasM], \n",
    "                   [MasF, FasF]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "confusion_matrix",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2) [[26.  1.]\n",
      " [ 1.  5.]]\n"
     ]
    }
   ],
   "source": [
    "cfm = np.zeros((2,2))\n",
    "\n",
    "for i in range(len(X_bio)):\n",
    "    cfm[naive_bayes_classifier(*X_bio[i], *p), int(y_bio[i])] += 1\n",
    "\n",
    "print(cfm.shape, cfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the A Posteriori Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier as defined above is only capable of deciding $X=0$ or $X=1$. In many practical applications it is important to know how probable that classification really is. I.e. you would like to calculate the a posteriori probability $P(X=x\\bigm|W=w,L=l,S=s)$ where $x$ is the classification result. Can you upgrade your classification function not only to return $x$ but also the a posteriori probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First rewrite the a posteriori probability into a form using only probabilities and probability densities that we are able to calculate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "a_posteriori",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "\\begin{align}\n",
    "P(X=x \\bigm| W=w, L=l, S=s) &= \\frac{P(W\\approx w, L\\approx l, S\\approx s\\bigm| X=x)P(X=x)}{P(W\\approx w, L\\approx l, S\\approx s)}\\\\\n",
    "&= \\frac{P(W\\approx w, L\\approx l, S\\approx s\\bigm| X=x)P(X=x)}{\n",
    "P(W\\approx w, L\\approx l, S\\approx s\\bigm| X=1)P(X=1) + P(W\\approx w, L\\approx l, S\\approx s\\bigm| X=0)P(X=0)}\\\\\n",
    "&= \\frac{f_{WLS|X=1}(w,l,s)P(X=x)}{f_{WLS|X=1}(w,l,s)P(X=1) + f_{WLS|X=0}(w,l,s)P(X=0)}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When implementing your formula for $P(X=x\\bigm| W=w, L=w, S=s)$ again you should choose $P(X=1)=P(X=0)=0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "a_posteriori_code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def naive_bayes_classifier_prob(w, l, s, meansF, stdsF, meansM, stdsM):\n",
    "    \"\"\"Return a tuple (g, p) where g is the classification result (i.e. 1 in case the classifier decides the person with \n",
    "    weight=w, length=l and shoesize=s is a female, 0 otherwise) and p is the a posterio probability\"\"\"\n",
    "    pF = fNormal(w, meansF[0], stdsF[0]) * fNormal(l, meansF[1], stdsF[1]) * fNormal(s, meansF[2], stdsF[2])\n",
    "    pM = fNormal(w, meansM[0], stdsM[0]) * fNormal(l, meansM[1], stdsM[1]) * fNormal(s, meansM[2], stdsM[2])\n",
    "\n",
    "    if (pF > pM):\n",
    "        return (1, pF / (pF + pM))\n",
    "    else:\n",
    "        return (0, pM / (pF + pM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out all classification results and their probability and observe that the two errors are made with great confidence... (showing of course our set of features is way too small to make a correct classification):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 60. 183.  42.] 0.0 0 0.9465445826257338 correct\n",
      "[130. 185.  45.] 0.0 0 0.9999976068398813 correct\n",
      "[ 84. 180.  42.] 0.0 0 0.9756245704232823 correct\n",
      "[ 65. 195.  45.] 0.0 0 0.9999860895465281 correct\n",
      "[ 55. 170.  42.] 0.0 1 0.9394868584304261 wrong\n",
      "[ 63. 173.  41.] 1.0 1 0.9429488484604464 correct\n",
      "[ 74. 185.  43.] 0.0 0 0.9982039836819794 correct\n",
      "[ 50. 170.  38.] 1.0 1 0.9999985785056024 correct\n",
      "[ 82. 180.  42.] 1.0 0 0.9712144742173608 wrong\n",
      "[ 77. 190.  44.] 0.0 0 0.9999325274958734 correct\n",
      "[ 76. 179.  43.] 0.0 0 0.983731092093668 correct\n",
      "[ 66. 178.  42.] 0.0 0 0.7982347150239687 correct\n",
      "[ 79. 186.  45.] 0.0 0 0.999851602646076 correct\n",
      "[ 76. 189.  44.] 0.0 0 0.9998911547571083 correct\n",
      "[ 75. 175.  42.] 0.0 0 0.7160242712476723 correct\n",
      "[ 90. 190.  46.] 0.0 0 0.9999900874466343 correct\n",
      "[ 70. 180.  41.] 0.0 0 0.6410594709841666 correct\n",
      "[ 46. 164.  36.] 1.0 1 0.9999999999618033 correct\n",
      "[ 98. 189.  44.] 0.0 0 0.9999835965276344 correct\n",
      "[ 79. 180.  43.] 0.0 0 0.9914642408006165 correct\n",
      "[ 67. 189.  44.] 0.0 0 0.9997689252213131 correct\n",
      "[ 81.  190.   45.3] 0.0 0 0.9999766584880919 correct\n",
      "[ 85.  190.   44.5] 0.0 0 0.9999762866743495 correct\n",
      "[ 75. 188.  44.] 0.0 0 0.999824351795727 correct\n",
      "[ 70. 178.  43.] 0.0 0 0.9608019703570954 correct\n",
      "[ 60. 180.  43.] 0.0 0 0.9597352961818537 correct\n",
      "[ 73. 192.  44.] 0.0 0 0.9999569463089367 correct\n",
      "[ 76. 174.  42.] 0.0 0 0.6473388303106757 correct\n",
      "[ 78. 187.  44.] 0.0 0 0.9997976356487244 correct\n",
      "[ 75. 162.  38.] 1.0 1 0.9999995686471934 correct\n",
      "[ 50. 165.  37.] 1.0 1 0.9999999952176483 correct\n",
      "[ 76.  188.   44.5] 0.0 0 0.9998878968005066 correct\n",
      "[ 73. 180.  43.] 0.0 0 0.9859203127987285 correct\n"
     ]
    }
   ],
   "source": [
    "p = estimate_parameters(X_bio, y_bio)\n",
    "for f, t in zip(X_bio, y_bio):\n",
    "    g, prob = naive_bayes_classifier_prob(*f, *p)\n",
    "    if t==g:\n",
    "        label='correct'\n",
    "    else:\n",
    "        label='wrong'\n",
    "    print(f, t, g, prob, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a well known phenomenon that the naive Bayes classifier is quite good at classification, but not quite usable for estimating the a posteriori probabilities. In our data set the classifier was wrong in two cases, but the a posteriori probabilities are well over 90%."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "1010.4px",
    "left": "106px",
    "top": "110.8px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
