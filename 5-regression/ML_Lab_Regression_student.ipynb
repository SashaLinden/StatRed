{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML LabExercise --- Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from sklearn.datasets import load_iris\n",
    "import os, wget, sys\n",
    "\n",
    "def sr_download(file):\n",
    "    if not os.path.isfile(file):\n",
    "        url = \"https://staff.fnwi.uva.nl/r.vandenboomgaard/downloads/\" + file\n",
    "        print('[sr]',f'Downloading file {file} for statistical reasoning course')\n",
    "        wget.download(url)\n",
    "    else:\n",
    "        print('[sr]',f'data {file} already downloaded')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this LabExercise you have to program linear regression using a gradient descent procedure. We start with a simple straight line univariate regression. And then we generalize to multivariate linear regression and try to predict house prizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell some hidden $\\LaTeX$ definition.\n",
    "$\\renewcommand{\\t}{\\theta}$\n",
    "$\\newcommand{\\v}[1]{{\\boldsymbol #1}}$\n",
    "$\\newcommand{\\hv}[1]{\\widetilde{\\v #1}}$\n",
    "$\\newcommand{\\T}{^\\top}$\n",
    "$\\newcommand{\\inv}{^{-1}}$\n",
    "$\\newcommand{\\ls}[1]{{^{(#1)}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Toy Problem To Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting a First Order Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be looking at the classical problem of fitting a straight line through a lot of points. The resulting least squares solution has a closed form analytical solution. But in this lab we will focus on the machine learning perspective on this **regression problem** and solve the problem using an **iterative gradient descent** technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we generate test data. Because we generate the data, the 'true' value is known\n",
    "$$y_{\\text{true}} = 100 x + 30$$\n",
    "where $x$ takes values in the interval from $0$ to $10$.\n",
    "The observed values are\n",
    "$$y = y_\\text{true}  + y_\\text{noise}$$\n",
    "where $y_\\text{noise}$ is noise. For every observation the noise value will be different but comes from normal distributions that are independent and identically distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=66)\n",
    "x_1st = np.linspace(0,10,50)\n",
    "yt = 100 * x_1st + 30\n",
    "y_1st = yt + 100 * rng.standard_normal(*yt.shape)\n",
    "plt.plot(x_1st, yt, '-g', label='true values')\n",
    "plt.plot(x_1st, y_1st, '+b', label='noisy observations')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of linear regression is to find the functional relation between the $x$ and $y$ values given the noise data points. We start with a simple 1st order model for this relation, which we in this case know to be the true functional relation. In practical applications the model is only an assumption about the mathematical relation between the $x$ and $y$ values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analytical Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 1st order model for our data is\n",
    "$$h_{\\v\\t}(x) = \\t_0 + \\t_1 x$$\n",
    "Evidently given the way the data is generated we hope to estimate $\\v\\t=(\\t_0\\;\\t_1)^T = (30\\;100)^T$ from the noisy data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we make the augmented data matrix \n",
    "$$\\hv X = \\begin{pmatrix}1 & x\\ls 1\\\\ 1& x\\ls 2\\\\\\vdots & \\vdots\\\\1& x\\ls m\\end{pmatrix}$$ by concatenating a column of 1's with the vector of all data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_1st = np.vstack((np.ones_like(x_1st),x_1st)).T\n",
    "print(tX_1st[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analytical solution is given as:\n",
    "$$\\hat{\\v\\t} = \\left(\\hv X\\T \\hv X\\right)\\inv \\hv X\\T \\v y$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_1st = np.linalg.inv(tX_1st.T @ tX_1st) @ tX_1st.T @ y_1st\n",
    "print(theta_1st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,3))\n",
    "plt.plot(x_1st, yt, '-g', label='true function')\n",
    "plt.plot(x_1st, y_1st, '+b', label='noisy observations')\n",
    "plt.plot(x_1st, tX_1st @ theta_1st, '-r', label='estimated function')\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analytical solution is not too bad when comparing the plot of the known truth and the estimating model from the noisy data. Below you should calculate the cost of the analytical solution. The correct formula can be found in the lecture notes to be:\n",
    "$$J(\\hv\\t) = \\frac{1}{2 m} \\|\\hv X\\v\\t - \\v y\\|^2$$\n",
    "where $m$ is the number of examples used for estimation the parameter vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-10866222dbece467",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def calculate_cost(X, y, theta):\n",
    "    #. Your solution here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"the cost is: {calculate_cost(tX_1st, y_1st, theta_1st):5.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning we will often minimize a cost function to obtain the optimal parameters for the learning system. It is however seldom that a closed form analytical solution, like we have used in the previous section, can be found. In such cases we have to resort to a numerical technique to find those optimal parameter values.\n",
    "\n",
    "Read the [lecture notes](https://staff.fnwi.uva.nl/r.vandenboomgaard/MachineLearning/LectureNotes/Regression/LinearRegression/univariate_linear_regression.html#univariate-linear-regression) for a detailed explanation, in this LabExercise we only state the main results.\n",
    "\n",
    "The cost function associated with first order univariate linear regression is stated as\n",
    "$$J(\\v\\t) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( h_{\\v\\t}(x\\ls i) - y\\ls i \\right)^2$$\n",
    "where our parameterized model is\n",
    "$$h_{\\v\\t}(x) = \\t_0 + \\t_1 x$$\n",
    "\n",
    "Starting with some arbitrary guess for $\\v\\t$ the value of $\\v\\t$ is then iteratively updated by subtracting $\\alpha$ times the gradient vector:\n",
    "$$\\v\\theta := \\v\\theta -\n",
    "\\alpha \\frac{\\partial J(\\v\\theta)}{\\partial \\v\\theta}$$\n",
    "This gradient descent procedure is repeated until convergence (or most often in practice for a fixed number of iterations while keeping track of the cost function in the process)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting all learning examples in the augmented data matrix $\\hv X$ and target vector $\\v y$ we have seen that\n",
    "$$J(\\v\\t) = \\frac{1}{2m}\\|\\hv X\\v\\t-\\v y\\|^2$$\n",
    "The gradient is then given as\n",
    "$$\\frac{\\partial J(\\v\\theta)}{\\partial \\v\\theta} = \\frac{1}{m} \\hv X\\T (\\hv X \\v\\theta - \\v y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please complete the following functions in order to calculate the optimal $\\v \\theta$ vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a1cca11f49fe073d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def calculate_gradient(X, y, theta):\n",
    "    # calculate the gradient and return its value \n",
    "    # (in array of shape theta.shape)\n",
    "    # X: augmented data matrix\n",
    "    # y: target vector\n",
    "    # theta: parameter vector\n",
    "    #. Your solution here ...\n",
    "    return grad\n",
    "    \n",
    "    \n",
    "def gradient_descent(start_theta, alpha, n_iter, X, y, verbose=False, iter_fraction=10):\n",
    "    # update the theta vector n_iter times starting \n",
    "    # with theta = start_theta using learning speed alpha\n",
    "    # start_theta: starting value of parameter vector theta\n",
    "    # alpha: learning speed\n",
    "    # n_iter: no of iterations to be done\n",
    "    # X: augmented data matrix\n",
    "    # y: target vector\n",
    "    # verbose: True then print intermediate results\n",
    "    # iter_fraction: print at each 'n_iter/iter_fraction' iteration\n",
    "    # the function returns a tuple theta, costs\n",
    "    # where theta is the parameter vector and costs is an array\n",
    "    # of shape (n_iter,) that gives the result of `calculate_cost` \n",
    "    # after each iteration.\n",
    "    theta = start_theta\n",
    "    costs = np.zeros((n_iter))\n",
    "    for i in range(n_iter):\n",
    "        # calculate the new value of theta\n",
    "        #. Your solution here ...\n",
    "        if verbose:\n",
    "            if i%(n_iter//iter_fraction) == 0:\n",
    "                print(f\"Iteration {i:5d}, cost {calculate_cost(X, y, theta):10.3f}\")\n",
    "    if verbose:\n",
    "        print(f\"Iteration {i:5d}, cost {calculate_cost(X, y, theta):10.3f}\")\n",
    "    return theta, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_1st_gd, costs = gradient_descent(np.array([0,0]), 0.0001, 1000, tX_1st, y_1st, verbose=True) # this combination of parameters should work\n",
    "print(theta_1st_gd)\n",
    "plt.plot(costs)\n",
    "plt.xlabel('no of iterations')\n",
    "plt.ylabel('cost');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(x_1st, yt, '-g', label=f'true function $y=30+100x$') # the 'true' function\n",
    "plt.plot(x_1st, y_1st, '+b', label='noisy observations') # the noisy observations\n",
    "plt.plot(x_1st, tX_1st @ theta_1st, '-r', \n",
    "         label=f'analytical solution $y = {theta_1st[0]:5.2f} + {theta_1st[1]:5.2f}x$'); # analytical solution\n",
    "plt.plot(x_1st, tX_1st @ theta_1st_gd, '-k', \n",
    "         label=f'gradient descent solution $y = {theta_1st_gd[0]:5.2f} + {theta_1st_gd[1]:5.2f}x$'); # gradient descent solution\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Higher Order Polynomial Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we look at data where a first order model is too simplistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_true(x):\n",
    "    return 10 * (1/2 * (x - 5) ** 3 - 5 * (x - 5) + 4)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "xx_3rd = np.linspace(0, 10, 5000)\n",
    "yy_3rd = f_true(xx_3rd)\n",
    "plt.plot(xx_3rd, yy_3rd, lw=3, color='lightgray', label='true function')\n",
    "\n",
    "rng = np.random.default_rng(seed=6)\n",
    "x_3rd = rng.uniform(low=0, high=10, size=10)\n",
    "y_3rd = f_true(x_3rd) + 100 * rng.normal(size=x_3rd.shape)\n",
    "\n",
    "plt.plot(x_3rd, y_3rd, '+', label='noisy observations')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A note on the seed. In many notebooks the seed for the random number generator is set explicitly (in order to get reproducible results necessary for automated grading). You are however advised to try the code without setting the seed to get an impression of the randomness of the experiments.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider model functions $h_{\\v\\t}(x)$ of the form:\n",
    "$$h_{\\v\\t}^{(k)}(x) = \\sum_{n=0}^k \\t_n x^n$$\n",
    "for $k=0,1,3,5,9$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $x\\ls i$ be one of the examples in the learning set then the corresponding row in the augmented data matrix is dependent on the order of the model:\n",
    "$$k=0:\\qquad (1)$$\n",
    "$$k=1:\\qquad (1\\;x)$$\n",
    "$$k=2:\\qquad (1\\;x\\;x^2)$$\n",
    "etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function ``make_augmented_univariate_matrix`` that takes the **vector** of scalar learning examples $(x\\ls 1\\cdots x\\ls m)$ and returns the augmented data matrix for order ``k``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9fdb3f3fdba1df13",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def make_augmented_univariate_matrix(x, order):\n",
    "    #. Your solution here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit a 1st order model (k=1) to the data (using the analytical solution). Here we won't use the theoretical analytical solution with the inverse matrix:\n",
    "$$\\hat{\\v\\t} = \\left(\\hv X\\T \\hv X\\right)\\inv \\hv X\\T \\v y$$ \n",
    "But we will use the scipy.linalg.lstsq function that is optimized for a least squares optimization problem. The numerical stability is much better than using the inverse or even using the sci.linalg.solve function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "tX = make_augmented_univariate_matrix(x_3rd, k) # make augmented data matrix $\\tilde X$ (tX)\n",
    "# theta = scipy.linalg.solve(tX.T @ tX, tX.T @ y_3rd) # analytical solution for theta\n",
    "theta,_,_,_ = scipy.linalg.lstsq(tX, y_3rd) # we don't need the other return values.\n",
    "print(y_3rd-tX@theta)\n",
    "plt.figure(figsize=(15,5))\n",
    "# plot the high resolution (many increasing x values in xx array) true function (arrays defined in previous cell)\n",
    "plt.plot(xx_3rd, yy_3rd, lw=5, color='lightgray', label='true function')\n",
    "# plot the noisy observations\n",
    "plt.plot(x_3rd, y_3rd, '+', label='noisy observations')\n",
    "\n",
    "# calculate the estimated y values for all x values in xx\n",
    "tXX = make_augmented_univariate_matrix(xx_3rd, k)\n",
    "yy = tXX @ theta\n",
    "plt.plot(xx_3rd, yy, lw=2, label=f'estimated order {k} function')\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');\n",
    "plt.title(f'Cost = {calculate_cost(tX, y_3rd, theta):5.2f}'); ## assuming your calculate_cost is generic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make plots like the one above but for $k=0,1,3,5,7,9$. Make the legend readable and informative. For each $k$ also calculate the cost on the learning set and store it in array ``cost_learn`` (of shape ``(len(ks),)``). For each $k$ also store the $\\v\\t$ vector in a list ``theta_learn``. **We won't test/grade the plots, but we will test/grade the cost on the learning set as function of the order.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-208762fd77d38980",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mn = yy_3rd.min()\n",
    "mx = yy_3rd.max()\n",
    "ks = [0,1,3,5,7,9]\n",
    "cost_learn = np.zeros_like(ks)\n",
    "theta_learn = []\n",
    "fig, axs = plt.subplots(len(ks), sharex=True, figsize=(15,25));\n",
    "for i, k in enumerate(ks):\n",
    "    axs[i].plot(xx_3rd, yy_3rd, lw=5, color='lightgray', label='true function')\n",
    "    axs[i].plot(x_3rd, y_3rd, '+', label='noisy observations')\n",
    "    # calculate the estimator for the theta vector\n",
    "    # and plot the estimated values for each point x in the xx_3rd array\n",
    "    #. Your solution here ...\n",
    "    axs[i].legend()\n",
    "    axs[i].set_xlabel('x')\n",
    "    axs[i].set_ylabel('y');\n",
    "    axs[i].set_ylim((mn, mx))\n",
    "    cost_learn[i] = calculate_cost(X, y_3rd, theta)\n",
    "    theta_learn.append(theta)\n",
    "    axs[i].text(8, -200, f'Order {k}, Cost = {cost_learn[i]:5.2f}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we plot and print the cost on the learning set. You should be able to explain (not graded but a typical exam question) why the cost is decreasing even in case the order of our model is higher than the model used to generate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.stem(ks, cost_learn);\n",
    "for k, c in zip(ks, cost_learn):\n",
    "    print(k, c)\n",
    "plt.xlabel('k (order of model)')\n",
    "plt.ylabel('cost on learning set');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all went well you should be able to observe that the cost gets lower for increasing order. Even for orders above the 'true order' of 3. You should be able to explain why that is (not graded but a typical exam question)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will test our linear regression using new data (from the same 'true model' but with different noise added). We calculate the cost of fitting the models learned on the original data using the new data. This will give insight in the true performance of the models working on 'unseen' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=666)\n",
    "x_3rd_test = rng.uniform(low=0, high=10, size=200)\n",
    "y_3rd_test = f_true(x_3rd_test) + 100 * rng.normal(size=x_3rd_test.shape)\n",
    "\n",
    "mn = yy_3rd.min()\n",
    "mx = yy_3rd.max()\n",
    "\n",
    "ks = [0,1,3,5,7,9]\n",
    "cost_test = np.zeros_like(ks)\n",
    "\n",
    "fig, axs = plt.subplots(len(ks), 1, sharex=True, figsize=(15,25))\n",
    "\n",
    "for i, k in enumerate(ks):\n",
    "    axs[i].plot(xx_3rd, yy_3rd, lw=5, color='lightgray', label='true function')\n",
    "    axs[i].plot(x_3rd_test, y_3rd_test, '+', label='noisy observations')\n",
    "    #\n",
    "    # below we will use the theta_learn list you have made a few cells before\n",
    "    X = make_augmented_univariate_matrix(x_3rd_test, k)\n",
    "    theta = theta_learn[i]\n",
    "    XX = make_augmented_univariate_matrix(xx_3rd, k)\n",
    "    yy = XX @ theta\n",
    "    cost_test[i] = calculate_cost(X, y_3rd_test, theta)\n",
    "    axs[i].plot(xx_3rd, yy, lw=2, label=f'estimated order {k} function')\n",
    "    axs[i].legend()\n",
    "    axs[1].set_xlabel('x')\n",
    "    axs[i].set_ylabel('y');\n",
    "    plt.ylim((mn, mx));\n",
    "    axs[i].text(8, -200, f'Order {k} model, Cost = {cost_test[i]:5.2f}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.stem(ks, cost_test);\n",
    "for k, c in zip(ks, cost_test):\n",
    "    print(k, c)\n",
    "plt.xlabel('k (order of model)')\n",
    "plt.ylabel('cost on test set');\n",
    "plt.ylim([0,100000]) ## the cost gets enormous for larger orders..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing your results you should be able to answer the following questions (not tested/graded):\n",
    "1. is the cost for $k=3$ significantly different from the cost on the learning set or simply due to the random nature of the experiment\n",
    "2. what explains why the cost increases again for $k>3$?\n",
    "3. what are possible solutions for this problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start this lab please read the lecture notes to have an idea what logistic regression is all about. First of all logistic regression is a **classification** method. Whereas for linear regression an analytical closed form solution is known, this is *not* the case for logistic regression. Both 'regression' techniques lead to a convex function (with the learnable parameters as arguments) to be optimized and thus for both techniques the optimal parameter vector is unique. \n",
    "\n",
    "Computationally linear and logistic regression are very much alike (so the programming to be done in this section is limited)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Logistic Regression Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we wrap the logistic regression code into the `LogisticRegression` class. The typical way to use this class is the same as for classifiers from sklearn.\n",
    "\n",
    "    logregr = LogisticRegression()  # initialize \n",
    "    logregr.fit(X,y)\n",
    "    print(logregr.score(Xtest, ytest))\n",
    "    \n",
    "In the code below you have to fill in the blanks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f57d5580b65555f7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    \"\"\"\n",
    "    Logistic Regression.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.01, niter=1000, bias=True):\n",
    "        self.theta = None\n",
    "        self.niter = niter\n",
    "        self.alpha = alpha\n",
    "        self.bias = bias\n",
    "    \n",
    "    def g(self, v):\n",
    "        # the activation function (the logistic function)\n",
    "        #. Your solution here ...\n",
    "    \n",
    "    def fit(self, X, y, alpha=None):\n",
    "        m = len(y)\n",
    "                  \n",
    "        if self.bias:\n",
    "            # insert a bias columns of ones\n",
    "            tX = np.insert(X, 0, np.ones(m), axis=1)\n",
    "        else:\n",
    "            tX = X\n",
    "                  \n",
    "        if alpha is None:\n",
    "            alpha = self.alpha\n",
    "                  \n",
    "        if self.theta is None:\n",
    "            self.theta = np.zeros(tX.shape[1])\n",
    "            \n",
    "        def grad(X, y):\n",
    "            # gradient of the cost function\n",
    "            #. Your solution here ...\n",
    "            \n",
    "        self.training_cost = np.zeros(self.niter)\n",
    "        for i in range(self.niter):\n",
    "            # do one step in the gradient descent\n",
    "            #. Your solution here ...\n",
    "            self.training_cost[i] = self.cost(X, y)\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        if self.theta is None:\n",
    "            print(\"Error: first fit to data\")            \n",
    "\n",
    "        if self.bias:\n",
    "            tX = np.insert(X, 0, np.ones(X.shape[0]), axis=1)\n",
    "        else:\n",
    "            tX = X\n",
    "                  \n",
    "        return 1*(self.g(tX @ self.theta) > 0.5) # WHY THE MULTIPLICATION WITH 1??\n",
    "    \n",
    "    def cost(self, X, y):\n",
    "        if self.bias:\n",
    "            tX = np.insert(X, 0, np.ones(X.shape[0]), axis=1)\n",
    "        else:\n",
    "            tX = X\n",
    "            \n",
    "        #. Your solution here ...\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return 100 * sum( 1*(self.predict(X) == y)) / len(y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing logistic regression on a simple generated 2D example dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=98239)\n",
    "m = 200\n",
    "X0 = rng.normal(loc=(2, 2), scale=(1, 1), size=(m//2, 2))\n",
    "#randn(m//2,2) + [2,2] # note: m//2 is a division by 2, but different from m/2\n",
    "#X1 = randn(m//2,2) + [5,5]\n",
    "X1 = rng.normal(loc=(5, 5), scale=(1, 1), size=(m//2, 2))\n",
    "y0 = np.zeros(m//2)\n",
    "y1 = np.ones(m//2)\n",
    "X = np.vstack((X0, X1))\n",
    "y = np.hstack((y0, y1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the data gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], c=y, edgecolors='k', cmap=plt.cm.Paired);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here all data points in the 2D learning set are plotted as colored circles where the color indicates whether $y=0$ or $y=1$. Not we use the logistic regression class to learn the parameters for the classifier (how many parameters will there be in this case?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logregr = LogisticRegression(0.1, 5000)\n",
    "logregr.fit(X, y)\n",
    "print(f\"Score on the training set = {logregr.score(X,y):5.2f}%\")\n",
    "print(f\"Cost on the training set = {logregr.cost(X,y):5.3f}\")\n",
    "plt.plot(logregr.training_cost);\n",
    "plt.xlabel('Gradient descent iteration')\n",
    "plt.ylabel('Cost on the learning set');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a nicely decreasing cost as a function of the number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this score it is hard to conclude whether our logistic regression class is working well. It is always nice to be able to visualize the results especcially in a 2D space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization is done by sampling the 2D feature space in a square grid. For each of these $(x_1,x_2)$ values we calculate `logregr.predict(X)`. The results for all these points are collected in a large 2D array where each value is either 0 or 1 (the result of the prediction) and this array is displayed as an image. On top of that image we plot the points in our learning set. The points in the learning set are plotted in the color of the class they belong to. So a blue point on the brown background (or vice versa) is incorrectly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeLogRegr2D(logregr, X, y):    \n",
    "    # You will not be asked to reproduce this function by memorizing it...\n",
    "    xmin = X[:,0].min() - 0.5\n",
    "    xmax = X[:,0].max() + 0.5\n",
    "    ymin = X[:,1].min() - 0.5\n",
    "    ymax = X[:,1].max() + 0.5\n",
    "    mx, my = np.meshgrid( np.arange(xmin, xmax, 0.1), np.arange(ymin, ymax, 0.1) )\n",
    "    XX = np.c_[mx.ravel(), my.ravel()]\n",
    "    Z = logregr.predict(XX)\n",
    "    Z = Z.reshape(mx.shape)\n",
    "    plt.pcolormesh(mx, my, Z, cmap=plt.cm.Paired, shading='auto');\n",
    "    plt.scatter(X[:,0], X[:,1], c=y, edgecolors='k', cmap=plt.cm.Paired);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeLogRegr2D(logregr, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing your code with code that is known to work is always a good idea. In this case we will compare with the LogisticRegression from sklearn. Note that in practice such a comparison is often available; many scientists and engineers are working on improving algorithms and so existing less efficient solutions are available to compare your results with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as skLogisticRegression\n",
    "sklogregr = skLogisticRegression(C=1, fit_intercept=True)\n",
    "sklogregr.fit(X,y)\n",
    "visualizeLogRegr2D(sklogregr, X, y)\n",
    "plt.title(\"sklearn\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the results from our own logistic regression is comparable to the result from Sklearn's version. Note that the decision 'surface' is quite different but the resulting classification is about the same. Read the sklearn documentation about the C parameter if you want to know the difference..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection we will use Logistic Regresssion to classify iris flowers. The 3 different types of flowers are to be distinguished using four features. For each of the 3 classes we have 50 examples in a learning set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()  # the dataset is part of sklearn and loaded this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.DESCR) # a description of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_iris = iris.data    # the data matrix\n",
    "y_iris = iris.target  # the target vector\n",
    "np.unique(y_iris)     # the classes are encoded with 0, 1 and 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice way of visualizing this 4D data set is to make scatter plots of $(x^{(i)}, x^{(j)})$, for all combinations of $i$ and $j$. Be sure to understand what is plotted in all the subplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "colors = np.array(['r','g','b'])\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        if i==j:\n",
    "            continue\n",
    "        plt.subplot2grid([4,4], [i,j])\n",
    "        plt.scatter(X_iris[:,i], X_iris[:,j], c=colors[y_iris], marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task now is to separate the red (class number 0) from the other classes (i.e. for this exercise we take class number 1 and class number 2 to be the same). To do the two class logistic regression you first have to make a new target vector `y_iris_0vs12` that is 1 for the rows in the X matrix corresponding with class 0 and 0 for classes 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4676a558ff2d12c3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Your code in this cell should calculate y_iris_0vs12\n",
    "#. Your solution here ...\n",
    "print(1 * True, 1 * False) # this is a hint to do the above in one line..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logregr_iris = LogisticRegression(0.1, 1000)\n",
    "logregr_iris.fit(X_iris, y_iris_0vs12)\n",
    "print(\"Score = \", logregr_iris.score(X_iris, y_iris_0vs12), \"%\")\n",
    "print(\"Cost  = \", logregr_iris.cost(X_iris, y_iris_0vs12))\n",
    "plt.plot(logregr_iris.training_cost);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Separating class 1 or 2 from the rest is a lot harder."
   ]
  }
 ],
 "metadata": {
  "author": "l",
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "449.631px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
